{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93043807-3b5c-44e4-9a17-2a4b7da8676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Greta Tengattini, \n",
    "Love,Indus\n",
    "\n",
    "Introduction:\n",
    "This script is designed to interface with the Facebook API by parsing HAR (HTTP Archive) files.\n",
    "It extracts ad-related data from Facebook Meta's ad library, processes the relevant details, and exports the structured data into both CSV and Excel formats. \n",
    "By automating this process, we can save hours of manual work, ensuring efficiency and accuracy in ad data collection.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "from haralyzer import HarParser, HarPage  # HAR file parsing (not used directly but can be useful)\n",
    "import json  # Handle JSON data\n",
    "import os  # File path operations\n",
    "import pandas as pd  # Data manipulation and storage\n",
    "import datetime as dt  # Handling date and time\n",
    "from bs4 import BeautifulSoup  # Parsing HTML content\n",
    "import xlsxwriter  # Excel file handling\n",
    "from datetime import datetime, timedelta  # Additional date utilities\n",
    "\n",
    "# Define input and output file paths\n",
    "name_string = 'Fansidea_com__20240325'  # Base name for the files\n",
    "filename = os.path.join(r'C:\\Users\\inste\\Downloads', name_string + '.har')  # HAR file path\n",
    "output_filename = os.path.join(r'C:\\Users\\inste\\Downloads', name_string + '.csv')  # CSV output path\n",
    "xlsx_filename = os.path.join(r'C:\\Users\\inste\\Downloads', name_string + '.xlsx')  # Excel output path\n",
    "\n",
    "# Load HAR file and parse JSON\n",
    "with open(filename, 'r', errors='replace') as f:\n",
    "    har = json.loads(f.read())\n",
    "\n",
    "# Initialize lists to store extracted ad data\n",
    "entry_list = []  # List of processed entries\n",
    "entry_counter = 0  # Track number of processed entries\n",
    "ads = []  # Store extracted ad details\n",
    "\n",
    "# Loop through each network request in the HAR file\n",
    "for entry in har['log']['entries']:\n",
    "    try:\n",
    "        url = entry['request']['url']  # Extract request URL\n",
    "        \n",
    "        # Identify Facebook ad library requests\n",
    "        if 'https://www.facebook.com/ads/library/async/search_ads/?' in url:\n",
    "            entry_list.append(entry_counter)  # Track the entry index\n",
    "            \n",
    "            # Extract response data and clean up Facebook-specific prefixes\n",
    "            items = json.loads(entry['response']['content']['text'].replace('for (;;);', ''))\n",
    "            elements = items['payload']['results']  # Extract relevant ad data\n",
    "            \n",
    "            # Loop through extracted ads\n",
    "            for element in elements:\n",
    "                results = {\n",
    "                    'adArchiveID': element[0]['adArchiveID'],  # Ad archive ID\n",
    "                    'ad_link': f'https://www.facebook.com/ads/library/?id={element[0][\"adArchiveID\"]}',  # Direct link to ad\n",
    "                    'startDate': dt.datetime.fromtimestamp(element[0]['startDate']).strftime('%Y-%m-%d'),  # Ad start date\n",
    "                    'endDate': dt.datetime.fromtimestamp(element[0]['endDate']).strftime('%Y-%m-%d'),  # Ad end date\n",
    "                    'pageID': element[0]['pageID'],  # Page ID running the ad\n",
    "                    'pageName': element[0]['pageName'],  # Page name\n",
    "                    'link_url': element[0]['snapshot']['link_url'],  # Ad link URL\n",
    "                    'cta_text': element[0]['snapshot']['cta_text'],  # Call to action text\n",
    "                }\n",
    "                \n",
    "                # Extract body text\n",
    "                try:\n",
    "                    results['body_text'] = element[0]['snapshot']['cards'][0]['body']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    try:\n",
    "                        results['body_text'] = BeautifulSoup(element[0]['snapshot']['body']['markup']['__html'], features='html.parser').get_text('\\n')\n",
    "                    except (KeyError, ValueError, IndexError):\n",
    "                        results['body_text'] = ''\n",
    "                \n",
    "                # Extract ad title\n",
    "                try:\n",
    "                    results['title'] = element[0]['snapshot']['cards'][0]['title']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    results['title'] = element[0]['snapshot'].get('title', '')\n",
    "                \n",
    "                # Extract ad caption\n",
    "                try:\n",
    "                    results['caption'] = element[0]['snapshot']['cards'][0]['caption']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    results['caption'] = element[0]['snapshot'].get('caption', '')\n",
    "                \n",
    "                # Extract ad link description\n",
    "                try:\n",
    "                    results['link_description'] = element[0]['snapshot']['cards'][0]['link_description']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    results['link_description'] = element[0]['snapshot'].get('link_description', '')\n",
    "                \n",
    "                # Extract image URL\n",
    "                try:\n",
    "                    results['original_image_url'] = element[0]['snapshot']['cards'][0]['original_image_url']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    results['original_image_url'] = element[0]['snapshot'].get('images', [{}])[0].get('original_image_url', '')\n",
    "                \n",
    "                # Extract video preview image URL\n",
    "                try:\n",
    "                    results['video_preview_image_url'] = element[0]['snapshot']['cards'][0]['video_preview_image_url']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    results['video_preview_image_url'] = element[0]['snapshot'].get('videos', [{}])[0].get('video_preview_image_url', '')\n",
    "                \n",
    "                # Extract video HD URL\n",
    "                try:\n",
    "                    results['video_hd_url'] = element[0]['snapshot']['cards'][0]['video_hd_url']\n",
    "                except (KeyError, ValueError, IndexError):\n",
    "                    results['video_hd_url'] = element[0]['snapshot'].get('videos', [{}])[0].get('video_hd_url', '')\n",
    "                \n",
    "                # Append extracted ad data to the list\n",
    "                ads.append(results)\n",
    "    except (KeyError, ValueError):\n",
    "        continue\n",
    "    \n",
    "    entry_list.append(entry_counter)\n",
    "    entry_counter += 1\n",
    "\n",
    "# Convert extracted data into a Pandas DataFrame\n",
    "df = pd.DataFrame.from_dict(ads)\n",
    "df['order'] = df.index + 1  # Assign sequential order\n",
    "\n",
    "# Calculate days the ad has been running\n",
    "df['daysRunning'] = (pd.to_datetime(df['endDate']) - pd.to_datetime(df['startDate'])).dt.days\n",
    "\n",
    "# Determine if ad is still running\n",
    "maxend = max(pd.to_datetime(df['endDate']))\n",
    "df['stillRunning'] = pd.to_datetime(df['endDate']) >= (maxend - timedelta(days=4))\n",
    "\n",
    "# Set up Excel preview formulas\n",
    "df['image_prev'] = '=image(' + xlsxwriter.utility.xl_col_to_name(df.columns.get_loc(\"original_image_url\")) + df.index.astype(str) + ')'\n",
    "df['vid_prev'] = '=image(' + xlsxwriter.utility.xl_col_to_name(df.columns.get_loc(\"video_preview_image_url\")) + df.index.astype(str) + ')'\n",
    "\n",
    "# Define export column order\n",
    "export_cols = ['pageName', 'order', 'ad_link', 'daysRunning', 'image_prev', 'vid_prev', 'body_text',  'title', 'caption', 'link_description', 'link_url', 'cta_text', 'startDate', 'endDate', 'pageID', 'original_image_url', 'video_preview_image_url', 'video_hd_url', 'stillRunning', 'adArchiveID']\n",
    "df = df[export_cols]\n",
    "\n",
    "# Export data to CSV\n",
    "df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Export data to Excel\n",
    "df.to_excel(xlsx_filename, index=False, engine='xlsxwriter')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
